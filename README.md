# ğŸŒ Arabic â†’ English Neural Machine Translation

A Transformer-based Neural Machine Translation (NMT) system that translates Arabic text to English using PyTorch.

![Python](https://img.shields.io/badge/Python-3.8+-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-Deep_Learning-red)
![NLP](https://img.shields.io/badge/NLP-Transformers-green)
![Status](https://img.shields.io/badge/Status-Complete-success)

## ğŸ¯ Project Overview

This project implements a **Neural Machine Translation (NMT)** model from **Arabic to English** using a **Transformer encoder-decoder architecture** in PyTorch. Built as a final project for Pattern Recognition course.

```
Arabic Input:  "Ù…Ø±Ø­Ø¨Ø§ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ"
     â†“
[Transformer Model]
     â†“
English Output: "Hello how are you"
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMER MODEL                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   ENCODER   â”‚                      â”‚   DECODER   â”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”‚ Multi-Head  â”‚                      â”‚ Masked      â”‚      â”‚
â”‚  â”‚ Self-Attn   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Self-Attn   â”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      Cross           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”‚ Feed        â”‚      Attention       â”‚ Cross-Attn  â”‚      â”‚
â”‚  â”‚ Forward     â”‚                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”‚ Feed        â”‚      â”‚
â”‚  â”‚ Add & Norm  â”‚                      â”‚ Forward     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚        â†‘                                    â†“              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Positional  â”‚                      â”‚ Linear +    â”‚      â”‚
â”‚  â”‚ Encoding    â”‚                      â”‚ Softmax     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚        â†‘                                    â†“              â”‚
â”‚  Arabic Input                         English Output       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ”¤ **Transformer Architecture** | Full encoder-decoder with multi-head attention |
| ğŸ“Š **Positional Encoding** | Sinusoidal encoding from original paper |
| ğŸ“š **Custom Vocabulary** | Word-level tokenization with special tokens |
| ğŸ”„ **Beam Search** | Advanced decoding for better translations |
| ğŸ“ˆ **Training Visualization** | Loss curves and attention maps |

## ğŸ“ Project Structure

```
arabic-english-nmt/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ translation_transformer.ipynb
â”œâ”€â”€ data/
â”‚   â””â”€â”€ .gitkeep (add ara_.txt here)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ .gitkeep (saved model weights)
â””â”€â”€ src/
    â””â”€â”€ .gitkeep
```

## ğŸ› ï¸ Technical Details

### Model Components

| Component | Description |
|-----------|-------------|
| **Embedding** | Word embeddings for Arabic & English |
| **Positional Encoding** | Sinusoidal position information |
| **Multi-Head Attention** | Self-attention mechanism |
| **Feed Forward** | Position-wise feed-forward network |
| **Layer Normalization** | Stabilizes training |

### Special Tokens

| Token | Purpose |
|-------|---------|
| `<pad>` | Padding sequences |
| `<bos>` | Beginning of sentence |
| `<eos>` | End of sentence |
| `<unk>` | Unknown words |

### Hyperparameters

```python
# Model Configuration
D_MODEL = 256        # Embedding dimension
N_HEADS = 8          # Attention heads
N_LAYERS = 4         # Encoder/Decoder layers
D_FF = 512           # Feed-forward dimension
DROPOUT = 0.1        # Dropout rate
MAX_LEN = 100        # Maximum sequence length

# Training Configuration
BATCH_SIZE = 64
LEARNING_RATE = 0.0001
EPOCHS = 20
```

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install -r requirements.txt
```

### Required Libraries

```
torch>=1.9.0
numpy
pandas
matplotlib
tqdm
```

### Dataset

The model uses a parallel Arabic-English corpus (`ara_.txt`):
- Tab-separated format
- Column 0: English sentence
- Column 1: Arabic sentence

### Usage

1. **Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/arabic-english-nmt.git
cd arabic-english-nmt
```

2. **Add your dataset**
```bash
# Place ara_.txt in the data/ folder
```

3. **Run the notebook**
```bash
jupyter notebook notebooks/translation_transformer.ipynb
```

4. **Train the model**
```python
# Training is handled in the notebook
# Model will be saved to models/ folder
```

5. **Translate text**
```python
# After training
translate("Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù…ØµØ±")
# Output: "welcome to egypt"
```

## ğŸ“Š Training Pipeline

```
1. Load Dataset
      â†“
2. Preprocess Text
      â†“
3. Build Vocabularies
      â†“
4. Numericalize (Tokens â†’ IDs)
      â†“
5. Create DataLoaders
      â†“
6. Initialize Transformer
      â†“
7. Train with Cross-Entropy Loss
      â†“
8. Evaluate with BLEU Score
      â†“
9. Inference with Beam Search
```

## ğŸ“ˆ Results

| Metric | Score |
|--------|-------|
| Training Loss | ~ |
| Validation Loss | ~ |
| BLEU Score | ~ |

*Results depend on dataset size and training duration*

## ğŸ” Example Translations

| Arabic | English (Predicted) |
|--------|---------------------|
| Ù…Ø±Ø­Ø¨Ø§ | hello |
| ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ | how are you |
| Ø£Ù†Ø§ Ø·Ø§Ù„Ø¨ | i am a student |
| Ù…ØµØ± Ø¬Ù…ÙŠÙ„Ø© | egypt is beautiful |

## ğŸ“š References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer Paper
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Visual Guide
- PyTorch Documentation

## ğŸ“ Course Information

- **Course**: Pattern Recognition
- **Project**: Neural Machine Translation
- **University**: Helwan University

## ğŸ‘¨â€ğŸ’» Author

**Joox**
- IoT & AI Developer @ VoltX
- CS Student @ Helwan University '27





<p align="center">
  <b>â­ Star this repo if you find it useful!</b>
</p>
